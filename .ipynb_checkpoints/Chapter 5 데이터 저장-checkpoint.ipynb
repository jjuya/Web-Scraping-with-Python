{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 미디어 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참조를 저장하는 것\n",
    "    - 파일이 위치한 URL을 저장하기만 하면 된다\n",
    "    - 장점\n",
    "        - 스크레이퍼가 파일을 내려받을 필요가 없으므로 훨씬 빨리 동작, 대역폭도 적게 요구\n",
    "        - URL만 저장하므로 컴퓨터의 공간도 확보\n",
    "        - URL만 저장하고 파일을 내려받지 않는 코드는 만들기 쉽다\n",
    "        - 큰 파일을 내려받지 않으므로 호스트 서버의 부하도 적다\n",
    "    - 단점\n",
    "        - 핫링크(URL을 웹사이트나 앱에 포함)의 말썽이 생길 소지가 많다\n",
    "        - 공개된 블로그 같은 곳에 핫링크 이미지를 쓰면 난처한 일이 생길 수 있다.\n",
    "        - 앱에 사용할 미디어 파일을 다른 사람의 서버에 맡기고 싶지 않을 수 있다.\n",
    "        - 외부의 있는 파일은 바뀔 수 있다.\n",
    "        - 나중에 저장하려 했는데, 막상 그 파일이 사라졌거나 다른 것으로 바뀔 수 있다.\n",
    "        - 실제 웹 브라우저는 페이지의 HTML만 요청하지 않고, 거기 포함된 파일도 내려 받는다.\n",
    "        - 스크래이퍼에서 파일을 내려받으면 실제 사람이 사이트를 보는 것 처럼 보일 수 있고, 이것이 장점일 때가 있다.\n",
    "- 파일 자체를 내려 받는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원격 URL의 파일을 내려 받을 때\n",
    "- urllib.request.urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://www.pythonscraping.com\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "imageLocation = bsObj.find(\"a\", {\"id\":\"logo\"}).find(\"img\")[\"src\"]\n",
    "urlretrieve(imageLocation, \"logo.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### src속성에 연결되어 있는 내부파일 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "downloadDirectory = \"downloaded\"\n",
    "baseUrl = \"http://pythonscraping.com\"\n",
    "\n",
    "def getAbsoluteURL(baseUrl, source):\n",
    "    if source.startswith(\"http://www.\"):\n",
    "        url = \"http://\"+source[11:]\n",
    "    elif source.startswith(\"http://\"):\n",
    "        url = source\n",
    "    elif source.startswith(\"www.\"):\n",
    "        url = source[4:]\n",
    "        url = \"http://\"+source\n",
    "    else:\n",
    "        url = baseUrl+\"/\"+source\n",
    "    if baseUrl not in url:\n",
    "        return None\n",
    "    return url\n",
    "\n",
    "def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):\n",
    "    path = absoluteUrl.replace(\"www.\", \"\")\n",
    "    path = path.replace(baseUrl, \"\")\n",
    "    path = downloadDirectory+path\n",
    "    directory = os.path.dirname(path)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    return path\n",
    "\n",
    "html = urlopen(\"http://www.pythonscraping.com\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "downloadList = bsObj.findAll(src=True)\n",
    "\n",
    "for download in downloadList:\n",
    "    fileUrl = getAbsoluteURL(baseUrl, download[\"src\"])\n",
    "    if fileUrl is not None:\n",
    "        print(fileUrl)\n",
    "        urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
